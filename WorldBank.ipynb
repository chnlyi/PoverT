{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liang/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Option2 Not Standardized\\n# convert to numeric arrays\\nA = pd.concat([A_train,A_test], axis=0)\\nA.columns = [''.join(col).strip() for col in A.columns.values]\\nA_feature = A.drop(['poor','country'], axis=1)\\n# Encoding categorical cols\\nAX_all = pd.get_dummies(A_feature,drop_first=True)\\n# Split back into train and test\\nAX_test = AX_all[AX_all.index.isin(A_test.index)].as_matrix()\\nAX = AX_all[AX_all.index.isin(A_train.index)].as_matrix()\\n# Get train target \\nAy = np.array(A.poor[A.poor.isnull()==False].astype(int))     \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "A_hhold_train = pd.read_csv('A_hhold_train.csv')\n",
    "A_indiv_train = pd.read_csv('A_indiv_train.csv')\n",
    "A_hhold_test = pd.read_csv('A_hhold_test.csv')\n",
    "A_indiv_test = pd.read_csv('A_indiv_test.csv')\n",
    "# remove cols\n",
    "A_indiv_train_1 = A_indiv_train.drop(['iid','poor','country'], axis=1)\n",
    "A_indiv_test_1 = A_indiv_test.drop(['iid','country'], axis=1)\n",
    "# indiv numerical cols\n",
    "A_indiv_train_num_col = list(A_indiv_train_1.dtypes[A_indiv_train_1.dtypes!='object'].index)\n",
    "A_indiv_train_num_col.remove('id')\n",
    "# keep categorial cols only\n",
    "A_indiv_train_cat = A_indiv_train_1.drop(A_indiv_train_num_col, axis=1)\n",
    "A_indiv_test_cat = A_indiv_test_1.drop(A_indiv_train_num_col, axis=1)\n",
    "# keep numerical cols only\n",
    "A_indiv_train_num_col.append('id')\n",
    "A_indiv_train_num = A_indiv_train_1[A_indiv_train_num_col]\n",
    "A_indiv_test_num = A_indiv_test_1[A_indiv_train_num_col]\n",
    "# pivot to get freq for cat cols\n",
    "A_indiv_train_cat_frq = (A_indiv_train_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "A_indiv_test_cat_frq = (A_indiv_test_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "# mean for num cols\n",
    "A_indiv_train_num_mean = A_indiv_train_num.groupby('id').mean()\n",
    "A_indiv_test_num_mean = A_indiv_test_num.groupby('id').mean()\n",
    "# join to hhold \n",
    "A_train = A_hhold_train.set_index('id').join(A_indiv_train_cat_frq).join(A_indiv_train_num_mean)\n",
    "A_test = A_hhold_test.set_index('id').join(A_indiv_test_cat_frq).join(A_indiv_test_num_mean)\n",
    "# add missing freq cols\n",
    "for i in [i for i in A_indiv_test_cat_frq.columns if i not in A_indiv_train_cat_frq.columns]:\n",
    "    A_train[i] = 0\n",
    "for i in [i for i in A_indiv_train_cat_frq.columns if i not in A_indiv_test_cat_frq.columns]:\n",
    "    A_test[i] = 0   \n",
    "\n",
    "#'''\n",
    "# Option1 Standardized   \n",
    "# convert to numeric arrays\n",
    "A = pd.concat([A_train,A_test], axis=0)\n",
    "A.columns = [''.join(col).strip() for col in A.columns.values]\n",
    "A_feature = A.drop(['poor','country'], axis=1)\n",
    "# standardization\n",
    "A_cols_to_std = list(A_feature.dtypes[A_feature.dtypes!='object'].index)\n",
    "A_feature[A_cols_to_std] = A_feature[A_cols_to_std].apply(lambda x: (x-x.mean()) / x.std())\n",
    "# Encoding categorical cols\n",
    "AX_all = pd.get_dummies(A_feature,drop_first=True)\n",
    "# Split back into train and test\n",
    "AX_test = AX_all[AX_all.index.isin(A_test.index)].as_matrix()\n",
    "AX = AX_all[AX_all.index.isin(A_train.index)].as_matrix()\n",
    "# Get train target \n",
    "Ay = np.array(A.poor[A.poor.isnull()==False].astype(int))     \n",
    "\n",
    "#'''\n",
    "\n",
    "'''\n",
    "# Option2 Not Standardized\n",
    "# convert to numeric arrays\n",
    "A = pd.concat([A_train,A_test], axis=0)\n",
    "A.columns = [''.join(col).strip() for col in A.columns.values]\n",
    "A_feature = A.drop(['poor','country'], axis=1)\n",
    "# Encoding categorical cols\n",
    "AX_all = pd.get_dummies(A_feature,drop_first=True)\n",
    "# Split back into train and test\n",
    "AX_test = AX_all[AX_all.index.isin(A_test.index)].as_matrix()\n",
    "AX = AX_all[AX_all.index.isin(A_train.index)].as_matrix()\n",
    "# Get train target \n",
    "Ay = np.array(A.poor[A.poor.isnull()==False].astype(int))     \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8203, 346) (37560, 44) (37560, 39) (8203, 271) (37560, 3) (8203, 2) (8203, 621)\n"
     ]
    }
   ],
   "source": [
    "print(A_hhold_train.shape,A_indiv_train.shape,A_indiv_train_cat.shape,A_indiv_train_cat_frq.shape\n",
    "      ,A_indiv_train_num.shape,A_indiv_train_num_mean.shape,A_train.shape)\n",
    "print(A_hhold_test.shape,A_indiv_test.shape,A_indiv_test_cat.shape,A_indiv_test_cat_frq.shape\n",
    "      ,A_indiv_test_num.shape,A_indiv_test_num_mean.shape,A_test.shape)\n",
    "print(AX_all.shape,AX.shape,AX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liang/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#load\n",
    "B_hhold_train = pd.read_csv('B_hhold_train.csv')\n",
    "B_indiv_train = pd.read_csv('B_indiv_train.csv')\n",
    "B_hhold_test = pd.read_csv('B_hhold_test.csv')\n",
    "B_indiv_test = pd.read_csv('B_indiv_test.csv')\n",
    "# remove cols\n",
    "B_indiv_train_1 = B_indiv_train.drop(['iid','poor','country'], axis=1)\n",
    "B_indiv_test_1 = B_indiv_test.drop(['iid','country'], axis=1)\n",
    "# indiv numerical cols\n",
    "B_indiv_train_num_col = list(B_indiv_train_1.dtypes[B_indiv_train_1.dtypes!='object'].index)\n",
    "B_indiv_train_num_col.remove('id')\n",
    "# keep categorial cols only\n",
    "B_indiv_train_cat = B_indiv_train_1.drop(B_indiv_train_num_col, axis=1)\n",
    "B_indiv_test_cat = B_indiv_test_1.drop(B_indiv_train_num_col, axis=1)\n",
    "# keep numerical cols only\n",
    "B_indiv_train_num_col.append('id')\n",
    "B_indiv_train_num = B_indiv_train_1[B_indiv_train_num_col]\n",
    "B_indiv_test_num = B_indiv_test_1[B_indiv_train_num_col]\n",
    "# pivot to get freq for cat cols\n",
    "B_indiv_train_cat_frq = (B_indiv_train_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "B_indiv_test_cat_frq = (B_indiv_test_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "# mean for num cols\n",
    "B_indiv_train_num_mean = B_indiv_train_num.groupby('id').mean()\n",
    "B_indiv_test_num_mean = B_indiv_test_num.groupby('id').mean()\n",
    "# join to hhold \n",
    "B_train = B_hhold_train.set_index('id').join(B_indiv_train_cat_frq).join(B_indiv_train_num_mean,lsuffix='ind_')\n",
    "B_test = B_hhold_test.set_index('id').join(B_indiv_test_cat_frq).join(B_indiv_test_num_mean,lsuffix='ind_')\n",
    "# add missing freq cols\n",
    "for i in [i for i in B_indiv_test_cat_frq.columns if i not in B_indiv_train_cat_frq.columns]:\n",
    "    B_train[i] = 0\n",
    "for i in [i for i in B_indiv_train_cat_frq.columns if i not in B_indiv_test_cat_frq.columns]:\n",
    "    B_test[i] = 0   \n",
    "# convert to numeric arrays\n",
    "B = pd.concat([B_train,B_test], axis=0)\n",
    "B.columns = [''.join(col).strip() for col in B.columns.values]\n",
    "B_feature = B.drop(['poor','country'], axis=1)\n",
    "# standardization\n",
    "B_cols_to_std = list(B_feature.dtypes[B_feature.dtypes!='object'].index)\n",
    "B_feature[B_cols_to_std] = B_feature[B_cols_to_std].apply(lambda x: (x-x.mean()) / x.std())\n",
    "# Encoding categorical cols\n",
    "BX_all = pd.get_dummies(B_feature,drop_first=True)\n",
    "# Split back into train and test\n",
    "BX_test = BX_all[BX_all.index.isin(B_test.index)].as_matrix()\n",
    "BX = BX_all[BX_all.index.isin(B_train.index)].as_matrix()\n",
    "# Get train target \n",
    "By = np.array(B.poor[B.poor.isnull()==False].astype(int)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3255, 443) (20252, 227) (20252, 192) (3255, 1608) (20252, 33) (3255, 32) (3255, 2141)\n"
     ]
    }
   ],
   "source": [
    "print(B_hhold_train.shape,B_indiv_train.shape,B_indiv_train_cat.shape,B_indiv_train_cat_frq.shape\n",
    "      ,B_indiv_train_num.shape,B_indiv_train_num_mean.shape,B_train.shape)\n",
    "print(B_hhold_test.shape,B_indiv_test.shape,B_indiv_test_cat.shape,B_indiv_test_cat_frq.shape\n",
    "      ,B_indiv_test_num.shape,B_indiv_test_num_mean.shape,B_test.shape)\n",
    "print(BX_all.shape,BX.shape,BX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liang/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#load \n",
    "C_hhold_train = pd.read_csv('C_hhold_train.csv')\n",
    "C_indiv_train = pd.read_csv('C_indiv_train.csv')\n",
    "C_hhold_test = pd.read_csv('C_hhold_test.csv')\n",
    "C_indiv_test = pd.read_csv('C_indiv_test.csv')\n",
    "# remove cols\n",
    "C_indiv_train_1 = C_indiv_train.drop(['iid','poor','country'], axis=1)\n",
    "C_indiv_test_1 = C_indiv_test.drop(['iid','country'], axis=1)\n",
    "# indiv numerical cols\n",
    "C_indiv_train_num_col = list(C_indiv_train_1.dtypes[C_indiv_train_1.dtypes!='object'].index)\n",
    "C_indiv_train_num_col.remove('id')\n",
    "# keep categorial cols only\n",
    "C_indiv_train_cat = C_indiv_train_1.drop(C_indiv_train_num_col, axis=1)\n",
    "C_indiv_test_cat = C_indiv_test_1.drop(C_indiv_train_num_col, axis=1)\n",
    "# keep numerical cols only\n",
    "C_indiv_train_num_col.append('id')\n",
    "C_indiv_train_num = C_indiv_train_1[C_indiv_train_num_col]\n",
    "C_indiv_test_num = C_indiv_test_1[C_indiv_train_num_col]\n",
    "# pivot to get freq for cat cols\n",
    "C_indiv_train_cat_frq = (C_indiv_train_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "C_indiv_test_cat_frq = (C_indiv_test_cat.set_index('id').stack()\n",
    " .groupby(level=[0,1])\n",
    " .value_counts()\n",
    " .unstack(level=[1,2])\n",
    " .fillna(0)\n",
    " .sort_index(axis=1))\n",
    "# mean for num cols\n",
    "C_indiv_train_num_mean = C_indiv_train_num.groupby('id').mean()\n",
    "C_indiv_test_num_mean = C_indiv_test_num.groupby('id').mean()\n",
    "# join to hhold \n",
    "C_train = C_hhold_train.set_index('id').join(C_indiv_train_cat_frq).join(C_indiv_train_num_mean)\n",
    "C_test = C_hhold_test.set_index('id').join(C_indiv_test_cat_frq).join(C_indiv_test_num_mean)\n",
    "# add missing freq cols\n",
    "for i in [i for i in C_indiv_test_cat_frq.columns if i not in C_indiv_train_cat_frq.columns]:\n",
    "    C_train[i] = 0\n",
    "for i in [i for i in C_indiv_train_cat_frq.columns if i not in C_indiv_test_cat_frq.columns]:\n",
    "    C_test[i] = 0    \n",
    "# convert to numeric arrays\n",
    "C = pd.concat([C_train,C_test], axis=0)\n",
    "C.columns = [''.join(col).strip() for col in C.columns.values]\n",
    "C_feature = C.drop(['poor','country'], axis=1)\n",
    "# standardization\n",
    "C_cols_to_std = list(C_feature.dtypes[C_feature.dtypes!='object'].index)\n",
    "C_feature[C_cols_to_std] = C_feature[C_cols_to_std].apply(lambda x: (x-x.mean()) / x.std())\n",
    "# Encoding categorical cols\n",
    "CX_all = pd.get_dummies(C_feature,drop_first=True)\n",
    "# Split back into train and test\n",
    "CX_test = CX_all[CX_all.index.isin(C_test.index)].as_matrix()\n",
    "CX = CX_all[CX_all.index.isin(C_train.index)].as_matrix()\n",
    "# Get train target \n",
    "Cy = np.array(C.poor[C.poor.isnull()==False].astype(int))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6469, 165) (29913, 44) (29913, 36) (6469, 294) (29913, 6) (6469, 5) (6469, 464)\n"
     ]
    }
   ],
   "source": [
    "print(C_hhold_train.shape,C_indiv_train.shape,C_indiv_train_cat.shape,C_indiv_train_cat_frq.shape\n",
    "      ,C_indiv_train_num.shape,C_indiv_train_num_mean.shape,C_train.shape)\n",
    "print(C_hhold_test.shape,C_indiv_test.shape,C_indiv_test_cat.shape,C_indiv_test_cat_frq.shape\n",
    "      ,C_indiv_test_num.shape,C_indiv_test_num_mean.shape,C_test.shape)\n",
    "print(CX_all.shape,CX.shape,CX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AXtrn,AXtst,Aytrn,Aytst = train_test_split(AX,Ay,test_size=0.33, random_state=42)\n",
    "BXtrn,BXtst,Bytrn,Bytst = train_test_split(BX,By,test_size=0.33, random_state=42)\n",
    "CXtrn,CXtst,Cytrn,Cytst = train_test_split(CX,Cy,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, \n",
    "                               random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(AXtrn, Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 60,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 600}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid =  {'bootstrap': [False],\n",
    " 'max_depth': [70,80],\n",
    " 'max_features': [32,36],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [6,7],\n",
    " 'n_estimators': [200,400]}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 243 out of 243 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'bootstrap': [False], 'max_depth': [50, 60, 70], 'max_features': [24, 28, 32], 'min_samples_leaf': [1], 'min_samples_split': [4, 5, 6], 'n_estimators': [400, 600, 800]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 70,\n",
       " 'max_features': 32,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=70, max_features=32, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=6,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestrf = RandomForestClassifier()\n",
    "bestrf.set_params(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=70, max_features=32, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=6,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestrf.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ayprdbestrf = bestrf.predict(AXtst)\n",
    "Ayprdprobbestrf = bestrf.predict_proba(AXtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1284,  214],\n",
       "       [ 178, 1031]], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Aytst, Ayprdbestrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35940109445403479"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(Aytst, Ayprdprobbestrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=1, shrinking=True, tol=0.001,\n",
       "  verbose=False),\n",
       "          fit_params=None, iid=True, n_iter=20, n_jobs=4,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002418A6470B8>, 'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002418A6475C0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=2017, refit=True,\n",
       "          return_train_score='warn', scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC as svc \n",
    "from scipy import stats\n",
    " \n",
    "svc = svc(probability = True, random_state = 1)\n",
    "\n",
    "rand_list = {\"C\": stats.uniform(2, 10),\n",
    "             \"gamma\": stats.uniform(0.1, 1)}\n",
    "              \n",
    "svm_random = RandomizedSearchCV(svc, param_distributions = rand_list, n_iter = 20, \n",
    "                                n_jobs = 4, cv = 3, random_state = 2017, scoring = 'neg_log_loss' ) \n",
    "svm_random.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 9.0331975797816781, 'gamma': 1.0312187582625605}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=9.0331975797816781, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1.0312187582625605,\n",
       "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "bestsvc = SVC(probability=True)\n",
    "bestsvc.set_params(**svm_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=9.0331975797816781, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=1.0312187582625605,\n",
       "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestsvc.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ayprdbestsvc = bestsvc.predict(AXtst)\n",
    "Ayprdprobbestsvc= bestsvc.predict_proba(AXtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1498,    0],\n",
       "       [1209,    0]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Aytst, Ayprdbestsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68754074182439362"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(Aytst, Ayprdprobbestsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_test = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_test.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1294,  204],\n",
       "       [ 187, 1022]], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Aytst, svc_test.predict(AXtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3205624269636016"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(Aytst, svc_test.predict_proba(AXtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dask.diagnostics import ProgressBar\n",
    "param_grid =  [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5], 'C': [100, 300, 1000]}]\n",
    "                #{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "# Create a based model\n",
    "svc = SVC(probability=True)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv = 3, scoring = 'neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kernel': ['rbf'], 'gamma': [0.001, 0.0001, 1e-05], 'C': [100, 300, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "bestsvc = SVC(probability=True)\n",
    "bestsvc.set_params(**grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestsvc.fit(AXtrn,Aytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1296,  202],\n",
       "       [ 149, 1060]], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Aytst, bestsvc.predict(AXtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29884170686672307"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(Aytst, bestsvc.predict_proba(AXtst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(30, input_shape=(795,),kernel_initializer='normal', activation='relu'\n",
    "             ,kernel_regularizer=regularizers.l2(0.005)))\n",
    "nn.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4396 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "4396/4396 [==============================] - 3s 694us/step - loss: 0.2995 - acc: 0.8904 - val_loss: 0.4291 - val_acc: 0.8327\n",
      "Epoch 2/100\n",
      "4396/4396 [==============================] - 3s 686us/step - loss: 0.2965 - acc: 0.8901 - val_loss: 0.3781 - val_acc: 0.8518\n",
      "Epoch 3/100\n",
      "4396/4396 [==============================] - 3s 707us/step - loss: 0.2985 - acc: 0.8865 - val_loss: 0.3715 - val_acc: 0.8573\n",
      "Epoch 4/100\n",
      "4396/4396 [==============================] - 3s 691us/step - loss: 0.2982 - acc: 0.8888 - val_loss: 0.3691 - val_acc: 0.8536\n",
      "Epoch 5/100\n",
      "4396/4396 [==============================] - 3s 742us/step - loss: 0.2986 - acc: 0.8881 - val_loss: 0.3694 - val_acc: 0.8545\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241bfe07a90>"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='loss', min_delta=0.0005, patience=3, \n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "nn.fit(AXtrn, Aytrn, epochs=100, batch_size=3, verbose=1, callbacks=callbacks_list,\n",
    "                       validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ayprednn = nn.predict(AXtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1214,  284],\n",
       "       [ 125, 1084]], dtype=int64)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Aytst, (Ayprednn>=.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ayprednn_proba = np.concatenate((1-Ayprednn,Ayprednn),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32834306457796814"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(Aytst, Ayprednn_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
